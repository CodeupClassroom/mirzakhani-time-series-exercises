{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d569077",
   "metadata": {},
   "source": [
    "# Preparing Time Series Data\n",
    "Like most preparation processes, but now with dates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173858f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta, datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from env import user, password, host\n",
    "import os\n",
    "from acquire import wrangle_store_data\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# plotting defaults\n",
    "plt.rc('figure', figsize=(13, 7))\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rc('font', size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f97a15",
   "metadata": {},
   "source": [
    "## Acquire\n",
    "We'll start by loading up the data from mysql server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd83706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following are functions contained in the accompanying acquire.py file\n",
    "\n",
    "# def get_db_url(database):\n",
    "#     return f'mysql+pymysql://{user}:{password}@{host}/{database}'\n",
    "\n",
    "# def get_store_data():\n",
    "#     '''\n",
    "#     Returns a dataframe of all store data in the tsa_item_demand database and saves a local copy as a csv file.\n",
    "#     '''\n",
    "#     query = '''\n",
    "#     SELECT *\n",
    "#     FROM items\n",
    "#     JOIN sales USING(item_id)\n",
    "#     JOIN stores USING(store_id) \n",
    "#     '''\n",
    "#     \n",
    "#     df = pd.read_sql(query, get_db_url('tsa_item_demand'))\n",
    "#     \n",
    "#     df.to_csv('tsa_item_demand.csv', index=False)\n",
    "#     \n",
    "#     return df\n",
    "# \n",
    "# def wrangle_store_data():\n",
    "#     filename = 'tsa_store_data.csv'\n",
    "#     \n",
    "#     if os.path.isfile(filename):\n",
    "#         df = pd.read_csv(filename)\n",
    "#     else:\n",
    "#         df = get_store_data()\n",
    "#         \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdf4aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = wrangle_store_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6bd7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d1bb05",
   "metadata": {},
   "source": [
    "## Summarize\n",
    "Before we dive into the data preparation, we should get an idea of the shape of the data. We'll get a sense for the number of rows, column names, datatypes, descriptive statistics, number and percent of missing values. We'll also get a broad overview of the kind of data in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd945f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape # number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b4acbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853ac8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cb2bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() # number null values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2847e182",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.store_id.unique() # unique values of  store_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdf06de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.item_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6f3c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sale_date.nunique() # number of unique sales dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e05b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sale_date.nunique() / 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2adec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sale_date.min(), df.sale_date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe196d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sale_amount.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7e1ef5",
   "metadata": {},
   "source": [
    "#### Takeaways so far:\n",
    "\n",
    "- 913k rows\n",
    "- No missing values\n",
    "- `store_id` ranges from 1-10\n",
    "- `item_id` ranges from 1-50\n",
    "- 1826 unique days\n",
    "- `sale_amount` ranges from 0-231\n",
    "- `sale_date` is string and cannot be sorted appropriately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f992d52a",
   "metadata": {},
   "source": [
    "## Prepare\n",
    "The most common activity in preparing time series data is setting dates to datetime types using `pd.to_datetime`.\n",
    "\n",
    "Another common activity is looking at the frequency of the data and gaps in time or null values. As part of this, we'll investigate our data and figure out what each row represents and ask if that is what we want.\n",
    "\n",
    "### Set Datetime Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895cdd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reassign the sale_date column to be a datetime type\n",
    "df.sale_date = pd.to_datetime(df.sale_date)\n",
    "\n",
    "# Set the index as that date and then sort index (by the date)\n",
    "df = df.set_index(\"sale_date\").sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af86480",
   "metadata": {},
   "source": [
    "### Defining Our Observations\n",
    "Here we ask ourselves what each row (or observation) represents or measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3409b60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d45e38",
   "metadata": {},
   "source": [
    "Notice that each row is not a single day. With datetime indices, we can have multiple rows at the same datetime value. It looks like each individual row, that is, each observation, is a sales of a specific item at a specific store, one sale.\n",
    "\n",
    "We can confirm this by taking a look at the sale_id column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffb1f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape[0], df.sale_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782a58dd",
   "metadata": {},
   "source": [
    "It looks like each sale id does uniquely identify each row and each row is a combination of one item sold at one store. So each observation is the **quantity** of items sold on **one** day, in **one** store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db4f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate analysis of variable distributions\n",
    "for col in df.columns:\n",
    "    print('Column: ' + col)\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.hist(df[col])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea584855",
   "metadata": {},
   "source": [
    "### Let's get to know our data a bit better\n",
    "Currently, the dataframe has one row per sold item, but what if we need more or less granularity?\n",
    "\n",
    "Let's get to know our data then consider what it means to define our observation in different ways.\n",
    "\n",
    "We'll also look at different time-windows for our data.\n",
    "\n",
    "### Plot Target Variable Over Time\n",
    "Generate a quick plot of our target variable over time, the variable we wish to forecast or predict. We will first aggregate the `sale_amount` by `sale_date` using `sum()`, i.e. group by sale_date and sum the sale_amount. We will then do a quick plot of this aggregated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbac66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_date = df.groupby(['sale_date']).sale_amount.sum().reset_index()\n",
    "by_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c585f845",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_date.plot(x='sale_date', y='sale_amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c5d5f1",
   "metadata": {},
   "source": [
    "Is each store equally represented in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d01fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of observations per store = number of item-transactions per store.\n",
    "df.store_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f187a926",
   "metadata": {},
   "source": [
    "Is each item equally represented in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8761d673",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.item_id.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e730e16e",
   "metadata": {},
   "source": [
    "It looks like both items and stores are equally represented, that is, each store shows up the same number of times and so does each item.\n",
    "\n",
    "### Explore Stores + Items\n",
    "There are 50 different items and there were 18,260 sales for each item.\n",
    "\n",
    "We will explore the data behind the unique combinations of store & item.\n",
    "For example, what was the first sale date for each store/item combination? What was the last sale date?\n",
    "On how many different days was each item sold at each store?\n",
    "\n",
    "First, we will want to reset the index so that `sale_date` becomes a feature we can use in an aggregate.\n",
    "Next, we will group by `store_id` and `item_id`. Finally, we will perform the aggregates needed to answer the questions.\n",
    "\n",
    "Let's find the first sale date for each store/item combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bea486",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sale = df.reset_index().groupby(['store_id','item_id']).sale_date.min()\n",
    "first_sale.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8ca80b",
   "metadata": {},
   "source": [
    "Is Jan 1, 2013 the first date for all store/item combinations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a59908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unique dates from first_sale_per_storeitem\n",
    "first_sale.unique() # if there's only one unique value, then YES!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3403f785",
   "metadata": {},
   "source": [
    "What is the last (i.e. most recent) sale date for each store/item combination?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae5aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_sale = df.reset_index().groupby(['store_id','item_id']).sale_date.max()\n",
    "last_sale.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19db9a97",
   "metadata": {},
   "source": [
    "Is Dec 31, 2107 the last date for all store/item combinations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278a50ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_sale.unique() # if there's only one unique value, then YES!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5968bcfa",
   "metadata": {},
   "source": [
    "On how many different days was each item sold at each store?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9996534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_per_store_per_item = df.reset_index().groupby(['store_id','item_id']).sale_date.nunique()\n",
    "days_per_store_per_item.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2174a98f",
   "metadata": {},
   "source": [
    "Did every store sell at least one of every item on every day in the data's time span?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb6711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there's only one unique value in the list, then YES!\n",
    "days_per_store_per_item.unique() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6796bcf4",
   "metadata": {},
   "source": [
    "Every store sold every item on every one of the 1826 days in our data history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a0413a",
   "metadata": {},
   "source": [
    "## Check for time gaps in the data\n",
    "\n",
    "While there might not be nulls present in our data, the time observations may leave a gap that we need to be able to detect. We can check the number of rows vs. the number of days that should exist between the minimum and maximum dates to see if there are any missing days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785ee548",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rows:', df.index.nunique())\n",
    "n_days = df.index.max() - df.index.min() + pd.Timedelta('1d')\n",
    "print(f\"Number of days between first and last day:\", n_days)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654c720f",
   "metadata": {},
   "source": [
    "## Reproducibility\n",
    "We'll now encapsulate all the preparation work that we've done into a single function, and store that function in prepare.py so that we can reference it again later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd2653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_store_data(df):\n",
    "    # Reassign the sale_date column to be a datetime type\n",
    "    df.sale_date = pd.to_datetime(df.sale_date)\n",
    "\n",
    "    # Sort rows by the date and then set the index as that date\n",
    "    df = df.set_index(\"sale_date\").sort_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeca184",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "Splitting time series data into train, test, and validate sets is a little trickier than with previous data we have looked at. Because the data points have an order to them, we cannot simply assign each point randomly to train, validate, or test.\n",
    "\n",
    "Ideally all splits should contain one season's worth of data. There are several methods we can use to split our time series data:\n",
    "\n",
    "**Human-based**: use, for example, the last year in the dataset as test split (i.e. use an arbitrary rule based on some convention)\n",
    "\n",
    "**Percentage based**: use the last 20% as test\n",
    "\n",
    "**Cross Validate**: break data up into slices and use successive slices as train and test repeatedly (sklearn.model_selection.TimeSeriesSplit)\n",
    "\n",
    "### Percentage-Based Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e188ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = .70 #70% of the data\n",
    "n = df.shape[0]\n",
    "test_start_index = round(train_size * n)\n",
    "\n",
    "train = df[:test_start_index] # everything up (not including) to the test_start_index\n",
    "test = df[test_start_index:] # everything from the test_start_index to the end\n",
    "\n",
    "train_by_date = train.groupby(['sale_date']).sale_amount.sum().reset_index()\n",
    "#train_by_date.plot(x='sale_date', y='sale_amount')\n",
    "\n",
    "test_by_date = test.groupby(['sale_date']).sale_amount.sum().reset_index()\n",
    "#test_by_date.plot(x='sale_date', y='sale_amount')\n",
    "\n",
    "plt.plot(train_by_date.sale_date, train_by_date.sale_amount)\n",
    "plt.plot(test_by_date.sale_date, test_by_date.sale_amount)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75582c2",
   "metadata": {},
   "source": [
    "Uh oh. It appears that my percentage-based splitting method is arbitrarily doing something weird to the data. Lets take a closer look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f6dd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_by_date.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67447f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_by_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa80a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638438ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d8f51b",
   "metadata": {},
   "source": [
    "It appears that we are somewhat arbitrarily separating the train set from the test set in the middle of a single date. We can fix this by manually shifting the data over a tiny amount to round to the nearest date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9712b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[:'2016-07-01'] # everything up (not including) to the test_start_index\n",
    "test = df['2016-07-02':] # everything from the test_start_index to the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ff0e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1188918",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0e7179",
   "metadata": {},
   "source": [
    "Let's revisualize the split with the adjusted cutoff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbc7147",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_by_date = train.groupby(['sale_date']).sale_amount.sum().reset_index()\n",
    "#train_by_date.plot(x='sale_date', y='sale_amount')\n",
    "\n",
    "test_by_date = test.groupby(['sale_date']).sale_amount.sum().reset_index()\n",
    "#test_by_date.plot(x='sale_date', y='sale_amount')\n",
    "\n",
    "plt.plot(train_by_date.sale_date, train_by_date.sale_amount)\n",
    "plt.plot(test_by_date.sale_date, test_by_date.sale_amount)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9681cf40",
   "metadata": {},
   "source": [
    "## Human-Based Split\n",
    "\n",
    "We can arbitrarily decide to split based on the last year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e5ff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[:'2016'] # includes 2016\n",
    "test = df['2017']\n",
    "\n",
    "train_by_date = train.groupby(['sale_date']).sale_amount.sum().reset_index()\n",
    "#train_by_date.plot(x='sale_date', y='sale_amount')\n",
    "\n",
    "test_by_date = test.groupby(['sale_date']).sale_amount.sum().reset_index()\n",
    "#test_by_date.plot(x='sale_date', y='sale_amount')\n",
    "\n",
    "plt.plot(train_by_date.sale_date, train_by_date.sale_amount)\n",
    "plt.plot(test_by_date.sale_date, test_by_date.sale_amount)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e964a57c",
   "metadata": {},
   "source": [
    "## Cross-Validated Splitting Using `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa8c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "n_splits = 5\n",
    "tscv = TimeSeriesSplit(n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4dbfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in tscv.split(df):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    print(f\"Length of TRAIN: {len(train_index)}\", f\"Length of TEST: {len(test_index)}\", \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a49ad4",
   "metadata": {},
   "source": [
    "### Visualizing Cross-Validated Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab61105",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in tscv.split(df):\n",
    "    train = df[:train_index[-1]]\n",
    "    test = df[test_index[0]:test_index[-1]]\n",
    "\n",
    "    train_by_date = train.groupby(['sale_date']).sale_amount.sum().reset_index()\n",
    "    #train_by_date.plot(x='sale_date', y='sale_amount')\n",
    "\n",
    "    test_by_date = test.groupby(['sale_date']).sale_amount.sum().reset_index()\n",
    "    #test_by_date.plot(x='sale_date', y='sale_amount')\n",
    "\n",
    "    plt.plot(train_by_date.sale_date, train_by_date.sale_amount)\n",
    "    plt.plot(test_by_date.sale_date, test_by_date.sale_amount)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dec8eab",
   "metadata": {},
   "source": [
    "Notice that we still have the same issue we ran into before. If we want to use this method and we don't want to split in the middle of a day, we will need to either algorithmically or manually tweak the split points.\n",
    "\n",
    "# Exercises\n",
    "The end result of this exercise should be a file named `prepare.py`.\n",
    "\n",
    "**Using your store items data:**\n",
    "\n",
    "- Convert date column to datetime format.\n",
    "- Plot the distribution of sale_amount and item_price.\n",
    "- Set the index to be the datetime variable.\n",
    "- Add a 'month' and 'day of week' column to your dataframe.\n",
    "- Add a column to your dataframe, sales_total, which is a derived from sale_amount (total items) and item_price.\n",
    "- Make sure all the work that you have done above is reproducible. That is, you should put the code above into separate functions and be able to re-run the functions and get the same results.\n",
    "\n",
    "**Using OPS data (hint: if you don't know how to acquire this data, do a bit of google searching on how to do this with pandas):**\n",
    "- Acquire the Open Power Systems Data for Germany, which has been rapidly expanding its renewable energy production in recent years. The data set includes country-wide totals of electricity consumption, wind power production, and solar power production for 2006-2017. You can get the data here: https://raw.githubusercontent.com/jenfly/opsd/master/opsd_germany_daily.csv\n",
    "- Convert date column to datetime format.\n",
    "- Plot the distribution of each of your variables.\n",
    "- Set the index to be the datetime variable.\n",
    "- Add a month and a year column to your dataframe.\n",
    "- Fill any missing values.\n",
    "- Make sure all the work that you have done above is reproducible. That is, you should put the code above into separate functions and be able to re-run the functions and get the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77e4de6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
